
## 文本聚类

文本聚类简单点的来说就是将文本视作一个样本，在其上面进行聚类操作。但是与我们机器学习中常用的聚类操作不同之处在于。


我们的聚类对象不是直接的文本本身，而是文本提取出来的特征。因此如何提取特征因而是非常重要的一步。在HanLP中一共有三个文本聚类方法。前两种都基于词袋模式，第一个是最常见的聚类算法：k-means，但HanLP不光实现了k-means，还实现了速度更快效果更好的repeated bisection算法（重复二分法，还是翻译为累次平方法，好像是第一种）。笔者动笔前段时间刚刚添加了一个新的聚类分析器是，基于词向量的kmens聚类分析器。

基于词向量的kmeans聚类分析器，输入的需要时词向量化后的文件。虽然HanLP的词向量在Java实现中，还算可以，但在Python中使用不算太好用，同时Python也不推荐用HanLP做词向量，我们有更好的工具。所以这里我们也就不画蛇添足了。


而对于前两个聚类分析器而言，其聚类模块可以接受任意文本作为文档，而不需要用特殊分隔符隔开单词。另外，该模块还接受单词列表作为输入，用户可以将英文、日文等预先切分为单词列表后输入本模块。统计方法适用于所有语种，不必拘泥于中文。

** 分词器的性能问题 ** 

在repeated bisection算法无论性能还是速度都要优于kmens，但是在本人的测试中，前者速度基本原作者一致约为kmeans的三倍左右，但是性能略低于后者。此处请读者自行斟酌。

** 分词器的参数 ** 

自动判断聚类个数k(此处来自于[原文:HanLP中的文本聚类](https://github.com/hankcs/HanLP/wiki/%E6%96%87%E6%9C%AC%E8%81%9A%E7%B1%BB)
很多时候用户可能觉得聚类个数k这个超参数很难准确指定。在repeated bisection算法中，有一种变通的方法，那就是通过给准则函数的增幅设定阈值beta来自动判断k。此时算法的停机条件为，当一个簇的二分增幅小于beta时不再对该簇进行划分，即认为这个簇已经达到最终状态，不可再分；当所有簇都不可再分时，算法终止，此时产生的聚类数量就不再需要人工指定了。

在HanLP中，repeated bisection算法提供了3种接口，分别需要指定k、beta或两者同时指定。当同时指定k和beta时，满足两者的停止条件中任意一个算法都会停止。当只指定一个时，另一个停止条件不起作用。这三个接口列举如下：

```java
    public List<Set<K>> repeatedBisection(int nclusters)
    public List<Set<K>> repeatedBisection(double limit_eval)
    public List<Set<K>> repeatedBisection(int nclusters, double limit_eval)

```

当我们使用analyzer.repeatedBisection(1.0)时，可以进行自动聚类。


```python
from pyhanlp import * 
ClusterAnalyzer = SafeJClass('com.hankcs.hanlp.mining.cluster.ClusterAnalyzer')
analyzer = ClusterAnalyzer()
# 我们需要调用并返回自身

analyzer.addDocument("赵一", "流行, 流行, 流行, 流行, 流行, 流行, 流行, 流行, 流行, 流行, 蓝调, 蓝调, 蓝调, 蓝调, 蓝调, 蓝调, 摇滚, 摇滚, 摇滚, 摇滚");
analyzer.addDocument("钱二", "爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲");
analyzer.addDocument("张三", "古典, 古典, 古典, 古典, 民谣, 民谣, 民谣, 民谣");
analyzer.addDocument("李四", "爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 爵士, 金属, 金属, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲, 舞曲");
analyzer.addDocument("王五", "流行, 流行, 流行, 流行, 摇滚, 摇滚, 摇滚, 嘻哈, 嘻哈, 嘻哈");
analyzer.addDocument("马六", "古典, 古典, 古典, 古典, 古典, 古典, 古典, 古典, 摇滚");

print(analyzer.repeatedBisection(1.0))
```

    [[李四, 钱二], [王五, 赵一], [张三, 马六]]


### 评测

评测程序仍然使用搜狗文本分类语料库迷你版。过程为首先遍历子目录读取文档，以子目录+文件名作为id将文档传入聚类分析器进行聚类，并且计算F1值返回。该计算过程已被原作者封装为接口，我们可以直接调用


```python
CORPUS_FOLDER = "/home/fonttian/Data/CNLP/textClassification/sogou-mini/搜狗文本分类语料库迷你版"
for i in ["kmeans", "repeated bisection"]:
    print(i, ClusterAnalyzer.evaluate(CORPUS_FOLDER, i) * 100)
```

    kmeans 83.97065954968313
    repeated bisection 82.71523522720585

