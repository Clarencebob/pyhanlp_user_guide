{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 依存句法分析器\n",
    "\n",
    "在HanLP中一共有两种句法分析器\n",
    "* 依存句法分析\n",
    "    * [基于神经网络的高性能依存句法分析器](http://www.hankcs.com/nlp/parsing/neural-network-based-dependency-parser.html)\n",
    "    * [MaxEnt依存句法分析](http://www.hankcs.com/nlp/parsing/to-achieve-the-maximum-entropy-of-the-dependency-parser.html)\n",
    " \n",
    " ### 基于神经网络的高性能依存句法分析器\n",
    " \n",
    "HanLP中的基于神经网络的高性能依存句法分析器参考的是[14年Chen&Manning的论文（A Fast and Accurate Dependency Parser using Neural Networks）](https://cs.stanford.edu/~danqi/papers/emnlp2014.pdf)，[这里还有一个发在了Github的实现程序，其实现语言为Python](https://github.com/akjindal53244/dependency_parsing_tf)。除此之外，你还可以参考ljj123zz 的CSDN 一篇博客：[https://blog.csdn.net/ljj123zz/article/details/78834838](https://blog.csdn.net/ljj123zz/article/details/78834838)\n",
    "\n",
    "HanLP作者的原文介绍已经写得比较清楚，唯一要注意的是原文章中介绍的依存句法分析器为早期版本，输出的依存关系为英文，现在应该变为中文，而且从测试结果看，训练语料应该已经更新了，但是更新为了那个语料现在还不会是很清楚。\n",
    "\n",
    "\n",
    "### 基于最大熵的依存句法分析器\n",
    "\n",
    "经过测试这个句法分析器为真的很坑，绝对不建议使用,测试代码见最后，作者原文介绍请点击:[http://www.hankcs.com/nlp/parsing/to-achieve-the-maximum-entropy-of-the-dependency-parser.html](http://www.hankcs.com/nlp/parsing/to-achieve-the-maximum-entropy-of-the-dependency-parser.html)\n",
    "\n",
    "## 下面是使用的例子\n",
    "### 基于神经网络的高性能依存句法分析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t徐先生\t徐先生\tnh\tnr\t_\t4\t主谓关系\t_\t_\n",
      "2\t还\t还\td\td\t_\t4\t状中结构\t_\t_\n",
      "3\t具体\t具体\ta\tad\t_\t4\t状中结构\t_\t_\n",
      "4\t帮助\t帮助\tv\tv\t_\t0\t核心关系\t_\t_\n",
      "5\t他\t他\tr\tr\t_\t4\t兼语\t_\t_\n",
      "6\t确定\t确定\tv\tv\t_\t4\t动宾关系\t_\t_\n",
      "7\t了\t了\tu\tu\t_\t6\t右附加关系\t_\t_\n",
      "8\t把\t把\tp\tp\t_\t15\t状中结构\t_\t_\n",
      "9\t画\t画\tv\tv\t_\t8\t介宾关系\t_\t_\n",
      "10\t雄鹰\t雄鹰\tn\tn\t_\t9\t动宾关系\t_\t_\n",
      "11\t、\t、\twp\tw\t_\t12\t标点符号\t_\t_\n",
      "12\t松鼠\t松鼠\tn\tn\t_\t10\t并列关系\t_\t_\n",
      "13\t和\t和\tc\tc\t_\t14\t左附加关系\t_\t_\n",
      "14\t麻雀\t麻雀\tn\tn\t_\t10\t并列关系\t_\t_\n",
      "15\t作为\t作为\tv\tv\t_\t6\t动宾关系\t_\t_\n",
      "16\t主攻\t主攻\tv\tvn\t_\t17\t定中关系\t_\t_\n",
      "17\t目标\t目标\tn\tn\t_\t15\t动宾关系\t_\t_\n",
      "18\t。\t。\twp\tw\t_\t4\t标点符号\t_\t_\n",
      "\n",
      "徐先生 --(主谓关系)--> 帮助\n",
      "还 --(状中结构)--> 帮助\n",
      "具体 --(状中结构)--> 帮助\n",
      "帮助 --(核心关系)--> ##核心##\n",
      "他 --(兼语)--> 帮助\n",
      "确定 --(动宾关系)--> 帮助\n",
      "了 --(右附加关系)--> 确定\n",
      "把 --(状中结构)--> 作为\n",
      "画 --(介宾关系)--> 把\n",
      "雄鹰 --(动宾关系)--> 画\n",
      "、 --(标点符号)--> 松鼠\n",
      "松鼠 --(并列关系)--> 雄鹰\n",
      "和 --(左附加关系)--> 麻雀\n",
      "麻雀 --(并列关系)--> 雄鹰\n",
      "作为 --(动宾关系)--> 确定\n",
      "主攻 --(定中关系)--> 目标\n",
      "目标 --(动宾关系)--> 作为\n",
      "。 --(标点符号)--> 帮助\n",
      "\n",
      "徐先生 --(主谓关系)--> 帮助\n",
      "还 --(状中结构)--> 帮助\n",
      "具体 --(状中结构)--> 帮助\n",
      "帮助 --(核心关系)--> ##核心##\n",
      "他 --(兼语)--> 帮助\n",
      "确定 --(动宾关系)--> 帮助\n",
      "了 --(右附加关系)--> 确定\n",
      "把 --(状中结构)--> 作为\n",
      "画 --(介宾关系)--> 把\n",
      "雄鹰 --(动宾关系)--> 画\n",
      "、 --(标点符号)--> 松鼠\n",
      "松鼠 --(并列关系)--> 雄鹰\n",
      "和 --(左附加关系)--> 麻雀\n",
      "麻雀 --(并列关系)--> 雄鹰\n",
      "作为 --(动宾关系)--> 确定\n",
      "主攻 --(定中关系)--> 目标\n",
      "目标 --(动宾关系)--> 作为\n",
      "。 --(标点符号)--> 帮助\n",
      "\n",
      "麻雀 --(并列关系)--> \n",
      "雄鹰 --(动宾关系)--> \n",
      "画 --(介宾关系)--> \n",
      "把 --(状中结构)--> \n",
      "作为 --(动宾关系)--> \n",
      "确定 --(动宾关系)--> \n",
      "帮助 --(核心关系)--> \n",
      "##核心##\n"
     ]
    }
   ],
   "source": [
    "from pyhanlp import *\n",
    "# 依存句法分析\n",
    "sentence = HanLP.parseDependency(\"徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。\")\n",
    "\n",
    "print(sentence)\n",
    "\n",
    "for word in sentence.iterator():  # 通过dir()可以查看sentence的方法\n",
    "    print(\"%s --(%s)--> %s\" % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA))\n",
    "print()\n",
    "\n",
    "# 也可以直接拿到数组，任意顺序或逆序遍历\n",
    "word_array = sentence.getWordArray()\n",
    "for word in word_array:\n",
    "    print(\"%s --(%s)--> %s\" % (word.LEMMA, word.DEPREL, word.HEAD.LEMMA))\n",
    "print()\n",
    "\n",
    "# 还可以直接遍历子树，从某棵子树的某个节点一路遍历到虚根\n",
    "CoNLLWord = JClass(\"com.hankcs.hanlp.corpus.dependency.CoNll.CoNLLWord\")\n",
    "head = word_array[12]\n",
    "while head.HEAD:\n",
    "    head = head.HEAD\n",
    "    if (head == CoNLLWord.ROOT):\n",
    "        print(head.LEMMA)\n",
    "    else:\n",
    "        print(\"%s --(%s)--> \" % (head.LEMMA, head.DEPREL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最大熵依存句法分析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hankcs每天都在写程序\n",
      "1\thankcs\thankcs\tx\tx\t_\t6\t限定\t_\t_\n",
      "2\t每天\t每天\tr\tr\t_\t5\t施事\t_\t_\n",
      "3\t都\t都\td\td\t_\t5\t程度\t_\t_\n",
      "4\t在\t在\td\td\t_\t5\t程度\t_\t_\n",
      "5\t写\t写\tv\tv\t_\t0\t核心成分\t_\t_\n",
      "6\t程序\t程序\tn\tn\t_\t5\t内容\t_\t_\n",
      "\n",
      "吴彦祖每天都在写程序\n",
      "1\t吴彦祖\t吴彦祖\tn\tnr\t_\t5\t施事\t_\t_\n",
      "2\t每天\t每天\tr\tr\t_\t5\t施事\t_\t_\n",
      "3\t都\t都\td\td\t_\t5\t程度\t_\t_\n",
      "4\t在\t在\td\td\t_\t5\t程度\t_\t_\n",
      "5\t写\t写\tv\tv\t_\t0\t核心成分\t_\t_\n",
      "6\t程序\t程序\tn\tn\t_\t5\t内容\t_\t_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MaxEntDependencyParser = JClass(\"com.hankcs.hanlp.dependency.MaxEntDependencyParser\")\n",
    "\n",
    "print(\"hankcs每天都在写程序\")\n",
    "print(MaxEntDependencyParser.compute(\"hankcs每天都在写程序\"))\n",
    "print(\"吴彦祖每天都在写程序\")\n",
    "print(MaxEntDependencyParser.compute(\"吴彦祖每天都在写程序\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
